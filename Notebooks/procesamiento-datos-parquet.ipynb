{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.1.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"CeroFugas\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "conf = SparkConf().setAppName(app_name)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider')\n",
    "# conf.set('spark.hadoop.fs.s3a.access.key', <access_key>)\n",
    "# conf.set('spark.hadoop.fs.s3a.secret.key', <secret_key>)\n",
    "# conf.set('spark.hadoop.fs.s3a.session.token', <token>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "# Set the MinIO access key, secret key, endpoint, and other configurations\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "# Read a J\n",
    "\n",
    "df = spark.read.parquet(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\")\n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "\n",
    "df = spark.read.parquet(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configurar path de jars de Hadoop\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.5 pyspark-shell'\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-common:3.3.5,org.apache.hadoop:hadoop-aws:3.3.5\")\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "df = spark.read.format(\"parquet\").load(\"../DATASETS/reportes_agua_2024_01.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configurar variables de entorno para los JARs\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.5 pyspark-shell'\n",
    "\n",
    "# Crear configuración de Spark\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \n",
    "         \"org.apache.hadoop:hadoop-common:3.3.5,\"\n",
    "         \"org.apache.hadoop:hadoop-aws:3.3.5\")\n",
    "conf.set(\"spark.driver.extraClassPath\", \"hadoop-aws:3.3.5:aws-java-sdk-bundle-1.12.779.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\", \"hadoop-aws:3.3.5:aws-java-sdk-bundle-1.12.779.jar\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://tu-endpoint-minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leer el archivo\n",
    "df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Verificar variables de entorno\n",
    "print(\"JAVA_HOME:\", os.environ.get('JAVA_HOME'))\n",
    "print(\"SPARK_HOME:\", os.environ.get('SPARK_HOME'))\n",
    "print(\"HADOOP_HOME:\", os.environ.get('HADOOP_HOME'))\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configurar FindSpark si es necesario\n",
    "findspark.init()\n",
    "\n",
    "# Configurar variables de entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de paquetes más simple\n",
    "# Configuración de paquetes más completa\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--packages com.amazonaws:aws-java-sdk-bundle:1.12.779,'\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.5,'\n",
    "    'org.apache.hadoop:hadoop-client:3.3.5 '\n",
    "    'pyspark-shell'\n",
    ")\n",
    "\n",
    "# Ubicación de JARs adicionales\n",
    "hadoop_jars_path = r\"E:\\Spark\\spark-3.5.3-bin-hadoop3\\jars\"\n",
    "\n",
    "# Configuración de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Parquet Reader\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{hadoop_jars_path}/*\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{hadoop_jars_path}/*\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Leer el archivo\n",
    "    df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Ruta a tu instalación de Spark\n",
    "SPARK_HOME = r\"E:\\Spark\\spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "# Configurar variables de entorno\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--packages com.amazonaws:aws-java-sdk-bundle:1.12.262,'\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.5,'\n",
    "    'com.amazonaws:aws-java-sdk-bundle:1.12.779,'\n",
    "    'org.apache.hadoop:hadoop-client:3.3.5 pyspark-shell'\n",
    ")\n",
    "\n",
    "# Configuración de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Parquet Reader\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{SPARK_HOME}/jars/*\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{SPARK_HOME}/jars/*\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Leer el archivo\n",
    "    df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuración para conexión a cluster remoto\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.47.100:7077\") \\\n",
    "    .appName(\"MinIO Parquet Reader\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"GVTZ4kb69J0Gop6YaWSf\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"WDkCNivmHhMRrBPgOWRIHZSHb5iMNyzBJgSgFk4f\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.779,\" + \n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.5\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al leer el archivo: An error occurred while calling o43.read.\n",
      ": java.lang.IllegalStateException: LiveListenerBus is stopped.\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n",
      "\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiramishima\\AppData\\Local\\Temp\\ipykernel_12200\\470218523.py\", line 3, in <module>\n",
      "    df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
      "         ^^^^^^^^^^\n",
      "  File \"E:\\Spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\session.py\", line 1706, in read\n",
      "    return DataFrameReader(self)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py\", line 70, in __init__\n",
      "    self._jreader = spark._jsparkSession.read()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kiramishima\\.conda\\envs\\data_engineer\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\Spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kiramishima\\.conda\\envs\\data_engineer\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o43.read.\n",
      ": java.lang.IllegalStateException: LiveListenerBus is stopped.\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n",
      "\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Leer el archivo\n",
    "    df = spark.read.format(\"parquet\").load(\"s3a://cero-fugas/reportes_agua_2024_01.parquet\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
